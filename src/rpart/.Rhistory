require("data.table")
require("rpart")
require("rpart.plot")
install.packages("data.table")
knitr::opts_chunk$set(echo = TRUE)
# Matrices de entrenamiento y test
library('dplyr')
Xtrain <- train %>% select(!c(total_pr, ship_sp, cond, stock_photo))
library(dplyr)
library(GGally)
library(readr)
library(dplyr)
library(GGally)
library(glmnet)
df <- read_csv("mario.csv")
# Matrices de entrenamiento y test
library('dplyr')
Xtrain <- train %>% select(!c(total_pr, ship_sp, cond, stock_photo))
knitr::opts_chunk$set(echo = TRUE)
library(readr)
library(dplyr)
library(GGally)
library(glmnet)
df <- read_csv("mario.csv")
count_capital_letters <- function(str) {
capital_count <- sum(grepl("[A-Z]", strsplit(str, "")[[1]]))
return(capital_count)
}
count_words <- function(str) {
word_count <- length(strsplit(str, "\\s+")[[1]])
return(word_count)
}
# Add new column with capital letter count
# df$capital_count <- sapply(df$title, count_capital_letters)
# df$word_count <- sapply(df$title, count_words)
ggplot(df) + geom_boxplot(aes(x=total_pr)) + coord_flip()
# df_modelling <- df %>%
#   dplyr::select(-c(id, title)) %>%
#   filter(total_pr < 100)
df_modelling <- df %>%
mutate(ship_sp = ifelse(ship_sp == "ups3Day", "other", ship_sp)) %>%
dplyr::select(-c(id, title))
str(df)
set.seed(42)
test_size <- 0.3
sample <- sample(c(TRUE, FALSE), nrow(df_modelling), replace=TRUE, prob=c(1 - test_size, test_size))
train  <- df_modelling[sample, ]
test   <- df_modelling[!sample, ]
library(ggcorrplot)
ggcorrplot(cor(df_modelling %>%
dplyr::select(-c(cond,ship_sp, stock_photo))),
method = "circle",
type = "upper",
lab = TRUE,
lab_size = 2)
## CORRELACION NRO DE OFERTAS CON PRECIO INICIAL
## mas leve precio total con precio envio
# modelo con todas las vars
mod <- lm(total_pr ~ ., data = train)
summary(mod)
confint(mod)
mod2 <- lm(total_pr ~ ship_pr + wheels, data = train)
summary(mod2)
mod3 <- lm(total_pr ~ . - start_pr, data = train)
summary(mod3)
mod4 <- lm(total_pr ~ ship_pr * wheels, data = train)
summary(mod4)
mod5 <- lm(total_pr ~ . + ship_pr * wheels, data = train)
summary(mod5)
mod6 <- lm(total_pr ~ duration + n_bids + cond + ship_sp + ship_pr * wheels, data = train)
summary(mod6)
# SCE modelo sin inteaccion
sce2 <- deviance(mod2)
# SCE modelo con interacción
sce4 <- deviance(mod4)
# R2 parcial
(sce2 - sce4) / sce2
library(MASS)
mod_forward <- stepAIC(
object = lm(total_pr ~ 1, data = train), #punto de partida
scope = list(upper = lm(total_pr ~ ., data = train)), #máximo modelo posible
direction = "forward", #método de selección
trace = FALSE, #para no imprimir resultados parciales
k = 2, #penalización a emplear (2 = AIC, log(n) = BIC)
steps = 1000 #máximo nro de pasos (1000 es el default)
)
# Devuelve un objeto lm con el "mejor" modelo
summary(mod_forward)
mod_backward <- stepAIC(
object = lm(total_pr ~ ., data = train), #punto de partida
scope = list(upper = lm(total_pr ~ 1, data = train)), #máximo modelo posible
direction = "backward", #método de selección
trace = FALSE, #para no imprimir resultados parciales
k = 2, #penalización a emplear (2 = AIC, log(n) = BIC)
steps = 1000 #máximo nro de pasos (1000 es el default)
)
# Devuelve un objeto lm con el "mejor" modelo
summary(mod_backward)
mod_step <- stepAIC(
object = lm(total_pr ~ 1, data = train), #punto de partida
scope = list(upper = lm(total_pr ~ ., data = train)), #máximo modelo posible
direction = "both", #método de selección
trace = FALSE, #para no imprimir resultados parciales
k = 2, #penalización a emplear (2 = AIC, log(n) = BIC)
steps = 1000 #máximo nro de pasos
)
summary(mod_step)
var_stepwise <- names(coef(mod_step))[-1]
var_stepwise
mod7 <- lm(total_pr ~  n_bids  + ship_pr * wheels, data = train)
summary(mod7)
mod8 <- lm(total_pr ~ duration + n_bids + cond + ship_sp + ship_pr * wheels + wheels * n_bids + ship_pr * n_bids, data = train)
summary(mod8)
# mod9 <- lm(total_pr ~ cond + start_pr + seller_rate + stock_photo + wheels + capital_count + wheels * capital_count, data = train)
# summary(mod9)
# Crear función para calcular todo junto en un modelo
medidas <- function(m, modmax) {
# CME
cme <- sigma(m)^2
# R2aj
r2aj <- summary(m)$adj.r.squared
# PRESS
#press <- qpcR::PRESS(m)$stat
# cp
cp <- olsrr::ols_mallows_cp(m, modmax)
# aic
aic <- extractAIC(m)[2]
# bic
bic <- extractAIC(m, k = log(nrow(m$model)))[2]
# devolver
#c(cme = cme, r2aj = r2aj, press = press, cp = cp, aic = aic, bic = bic)
c(cme = cme, r2aj = r2aj, cp = cp, aic = aic, bic = bic)
}
# Ajustar todos los modelos y guardarlos en una lista
mod_lista <- list(mod2, mod3, mod4, mod5, mod6, mod7, mod8, mod_backward, mod_forward)
# Aplicar la función a cada elemento de la lista
sapply(mod_lista, medidas, modmax = mod)
mod_lista
# |dfbetas| > 2/sqrt(n) =  |dfbetas| > 0.207 --> observación ejerce influencia desmedida sobre la estimación de Bk
dfbetas <- dfbetas(mod8)
dfbetas <- as.data.frame(dfbetas)
dfbetas_filter <- filter(dfbetas, abs(dfbetas$`ship_pr:wheels`) > 0.207)
dfbetas_filter
train_id <- train %>% mutate(id = row_number())
filter(train_id, train_id$id == 11 | train_id$id == 55 |  train_id$id == 66 |  train_id$id == 69 |  train_id$id == 75 |  train_id$id == 76 )
#dfbetas(mod4)[31,2]
mod8 %>%
dfbetas() %>%
as_tibble() %>%
rename(Intercepto = 1, Pendiente = 4) %>%  #columna 4 = ship_pr:wheels
mutate(modelo = rownames(train)) %>%
ggplot() +
aes(x = Intercepto, y = Pendiente) +
geom_point() +
#geom_smooth(method=lm) +
geom_label(aes(label = modelo))
ggcorrplot(cor(train %>%
dplyr::select(c(duration, n_bids, ship_pr, wheels))),
method = "circle",
type = "upper",
lab = TRUE,
lab_size = 2)
plot(mod8)
df_residuals <- data.frame(n_obs = rownames(train),
rstandard = rstandard(mod8),
rstudent = rstudent(mod8),
hatvalues = hatvalues(mod8),
press = residuals(mod8)/(1-hatvalues(mod8)),
cook_distance = cooks.distance(mod8)
)
df_residuals
p <- 14 # incluyendo las interacciones
n <- train %>% nrow()
max_leverage <- 2*p/n
# puntos atipicos e influyentes
df_residuals %>%
filter(abs(rstandard) > 3 &
hatvalues > max_leverage &
cook_distance > 1)
df_residuals %>%
ggplot() +
geom_col(aes(x=n_obs, y=abs(rstandard)), fill = "blue")
df_residuals %>%
ggplot() + geom_col(aes(x=n_obs, y=abs(press)), fill = "red") +
geom_col(aes(x=n_obs, y=abs(rstandard)), fill = "blue")
# Matrices de entrenamiento y test
library('dplyr')
Xtrain <- train %>% select(!c(total_pr, ship_sp, cond, stock_photo))
# Matrices de entrenamiento y test
library('dplyr')
Xtrain <- train %>% dplyr::select(!c(total_pr, ship_sp, cond, stock_photo))
Ytrain <- train$total_pr
Xtest <- test %>% dplyr::select(!c(total_pr, ship_sp, cond, stock_photo))
Ytest <- test$total_pr
Xtrain <- as.matrix(Xtrain)
Ytrain <- as.matrix(train$total_pr)
# Matrices de entrenamiento y test
library('dplyr')
Xtrain <- train %>% dplyr::select(!c(total_pr, ship_sp, cond, stock_photo))
Ytrain <- train$total_pr
Xtest <- test %>% dplyr::select(!c(total_pr, ship_sp, cond, stock_photo))
Ytest <- test$total_pr
Xtrain <- as.matrix(Xtrain)
Ytrain <- as.matrix(train$total_pr)
Xtest <- as.matrix(Xtest)
Ytest <- as.matrix(Ytest)
# Evolución del error en función de lambda
# ==============================================================================
set.seed(123)
cv_error <- cv.glmnet(
x      = Xtrain,
y      = Ytrain,
alpha  = 0, #para ridge y/ 1 para lasso
nfolds = 5,
type.measure = "mse",
standardize  = TRUE
)
#plot(cv_error)
# Mejor valor lambda encontrado
# ==============================================================================
paste("Mejor valor de lambda encontrado:", cv_error$lambda.min)
# Ajustar el modelo
mod_ridge <- glmnet(Xtrain, Ytrain,
alpha = 0,
lambda = cv_error$lambda.min)
# Mostrar coeficientes
coef(mod_ridge)
# Predicciones de test
predicciones_test <- predict(mod_ridge, newx = Xtest)
# MSE de test
test_mse_ridge <- mean((predicciones_test - Ytest)^2)
paste("Error (mse) de test:", test_mse_ridge)
# Matrices de entrenamiento y test
library('dplyr')
#Xtrain <- train %>% dplyr::select(!c(total_pr, ship_sp, cond, stock_photo))
Xtrain <- train %>% dplyr::select(!c(total_pr))
Ytrain <- train$total_pr
#Xtest <- test %>% dplyr::select(!c(total_pr, ship_sp, cond, stock_photo))
Xtest <- test %>% dplyr::select(!c(total_pr))
Ytest <- test$total_pr
Xtrain <- as.matrix(Xtrain)
Ytrain <- as.matrix(train$total_pr)
Xtest <- as.matrix(Xtest)
Ytest <- as.matrix(Ytest)
View(Xtrain)
# Evolución del error en función de lambda
# ==============================================================================
set.seed(123)
cv_error <- cv.glmnet(
x      = Xtrain,
y      = Ytrain,
alpha  = 0, #para ridge y/ 1 para lasso
nfolds = 5,
type.measure = "mse",
standardize  = TRUE
)
# Ajustar el modelo
mod_ridge <- glmnet(Xtrain, Ytrain,
alpha = 0,
lambda = cv_error$lambda.min)
# Mostrar coeficientes
coef(mod_ridge)
# Matrices de entrenamiento y test
# ==============================================================================
x_train <- model.matrix(total_pr~., data = train)[, -1]
y_train <- train$total_pr
x_test <- model.matrix(total_pr~., data = test)[, -1]
y_test <- test$total_pr
# Evolución del error en función de lambda
# ==============================================================================
set.seed(123)
cv_error <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 0, #para ridge y/ 1 para lasso
nfolds = 5,
type.measure = "mse",
standardize  = TRUE
)
#plot(cv_error)
# Mejor valor lambda encontrado
# ==============================================================================
paste("Mejor valor de lambda encontrado:", cv_error$lambda.min)
# Ajustar el modelo
mod_ridge <- glmnet(x_train, y_train,
alpha = 0,
lambda = cv_error$lambda.min)
# Mostrar coeficientes
coef(mod_ridge)
# Predicciones de test
predicciones_test <- predict(mod_ridge, newx = x_test)
# MSE de test
test_mse_ridge <- mean((predicciones_test - y_test)^2)
paste("Error (mse) de test:", test_mse_ridge)
set.seed(123)
cv_error_lasso <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1, #para ridge y/ 1 para lasso
nfolds = 5)
#plot(cv_error)
# Mejor valor lambda encontrado
# ==============================================================================
paste("Mejor valor de lambda encontrado:", cv_error_lasso$lambda.min)
# Ajustar el modelo
mod_lasso <- glmnet(x_train, y_train,
alpha = 1,
lambda = cv_error_lasso$lambda.min)
# Mostrar coeficientes
round(coef(mod_lasso),4)
# Evolución del error en función de lambda
# ==============================================================================
set.seed(123)
cv_error <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 0, #para ridge y/ 1 para lasso
nfolds = 5,
type.measure = "mse",
standardize  = TRUE
)
plot(cv_error)
# Mejor valor lambda encontrado
# ==============================================================================
paste("Mejor valor de lambda encontrado:", cv_error$lambda.min)
# Ajustar el modelo
mod_lasso <- glmnet(x_train, y_train,
alpha = 1,
lambda = 5)
# Mostrar coeficientes
round(coef(mod_lasso),4)
set.seed(123)
cv_error_lasso <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1, #para ridge y/ 1 para lasso
nfolds = 5)
#plot(cv_error)
# Mejor valor lambda encontrado
# ==============================================================================
paste("Mejor valor de lambda encontrado:", log(cv_error_lasso$lambda.min))
# Ajustar el modelo
mod_lasso <- glmnet(x_train, y_train,
alpha = 1,
lambda = 2.94)
# Mostrar coeficientes
round(coef(mod_lasso),4)
set.seed(123)
cv_error_lasso <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1, #para ridge y/ 1 para lasso
nfolds = 10)
#plot(cv_error)
# Mejor valor lambda encontrado
# ==============================================================================
paste("Mejor valor de lambda encontrado:", log(cv_error_lasso$lambda.min))
set.seed(123)
cv_error_lasso <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1, #para ridge y/ 1 para lasso
nfolds = 10)
#plot(cv_error)
# Mejor valor lambda encontrado
# ==============================================================================
paste("Mejor valor de lambda encontrado:", cv_error_lasso$lambda.min)
set.seed(123)
cv_error_lasso <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1, #para ridge y/ 1 para lasso
nfolds = 5)
#plot(cv_error)
# Mejor valor lambda encontrado
# ==============================================================================
paste("Mejor valor de lambda encontrado:", cv_error_lasso$lambda.min)
git status
# Ajustar el modelo
mod_lasso <- glmnet(x_train, y_train,
alpha = 1,
lambda = cv_error_lasso$lambda.min)
# Mostrar coeficientes
round(coef(mod_lasso),4)
plot(mod_lasso, xvar = "lambda", label = TRUE)
plot(mod_ridge, xvar = "lambda", label = TRUE)
plot(cv_error_lasso)
plot(cv_error_lasso)
set.seed(100019)
cv_error_lasso <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1, #para ridge y/ 1 para lasso
nfolds = 5)
#plot(cv_error)
# Mejor valor lambda encontrado
# ==============================================================================
paste("Mejor valor de lambda encontrado:", cv_error_lasso$lambda.min)
set.seed(1)
cv_error_lasso <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1, #para ridge y/ 1 para lasso
nfolds = 5)
#plot(cv_error)
# Mejor valor lambda encontrado
# ==============================================================================
paste("Mejor valor de lambda encontrado:", cv_error_lasso$lambda.min)
set.seed(123)
cv_error_lasso <- cv.glmnet(
x      = x_train,
y      = y_train,
alpha  = 1, #para ridge y/ 1 para lasso
nfolds = 5)
#plot(cv_error)
# Mejor valor lambda encontrado
# ==============================================================================
paste("Mejor valor de lambda encontrado:", cv_error_lasso$lambda.min)
